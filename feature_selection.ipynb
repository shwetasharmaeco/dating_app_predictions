{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shwetasharma1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shwetasharma1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use(\"ggplot\")\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "np.random.seed(27)\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "sns.set(style=\"darkgrid\")\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "punctuation_ = set(string.punctuation)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer_porter = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diet column has more than half nans. will not be using it.\n",
    "#income had many negative values. dropping tyhis column as well\n",
    "# users_no_na.groupby([\"ethnicity\"]).size()\n",
    "\n",
    "# users = pd.read_csv(\"./data/profiles.csv\")\n",
    "df = pd.read_csv(\"./users_essay\")\n",
    "# users_no_na = users[users[\"drugs\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'users_no_na' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a8942e0b4965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0musers_no_na\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musers_no_na\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'users_no_na' is not defined"
     ]
    }
   ],
   "source": [
    "col_lst = ['essay0','essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7','essay8', 'essay9']\n",
    "\n",
    "for col in col_lst:\n",
    "    users_no_na[col] = users_no_na[col].fillna(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_no_na[\"essay\"] = users_no_na[\"essay0\"] + users_no_na[\"essay1\"] + users_no_na[\"essay2\"] + users_no_na[\"essay3\"] + users_no_na[\"essay4\"] + users_no_na[\"essay5\"] + users_no_na[\"essay6\"] + users_no_na[\"essay7\"] + users_no_na[\"essay8\"] + users_no_na[\"essay9\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    clean the text data: split the text, remove the redundant signs and words.\n",
    "    '''\n",
    "    text = text.replace('<br />', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    words = nltk.word_tokenize(text)   #split the text into words\n",
    "    words = [word for word in words if word.isalpha()]    #remove the non-alphabetic signs\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in words if word not in set(stop_words)]   #remove the stop words\n",
    "    return words\n",
    "\n",
    "def clean_text_column(df, col_name):\n",
    "    '''\n",
    "    input a dataframe and one of its column, the function clean the text within\n",
    "    the column.\n",
    "    '''\n",
    "    df_ = df[df[col_name].notnull()]\n",
    "    df[col_name] = df_.apply(lambda row: clean_text(row[col_name]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_column(users_no_na, 'essay')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_no_na[\"essay\"].head()\n",
    "# users_no_na[\"essay\"] = users_no_na[\"essay\"].fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "users_no_na[\"essay\"] = users_no_na[\"essay\"].apply(lambda x: \", \".join(x))\n",
    "users_no_na[\"essay\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_accents(input_str):\n",
    "    '''\n",
    "    This function is to remove the accents\n",
    "    '''\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "def remove_http(sentences):\n",
    "    '''\n",
    "    remove the URLs in the text\n",
    "    '''\n",
    "    sentences_ = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "        sentences_.append(sentence)\n",
    "    return [sent for sent in sentences_ if not sent in {\"\", \"'\"}]\n",
    "\n",
    "def filter_tokens(sentence):\n",
    "    '''\n",
    "    This is to remove the stop words. Update the stop words in set stopwords2\n",
    "    '''\n",
    "    stopwords1 = set(stopwords.words('english'))\n",
    "    stopwords2 = {\"\\'s\",\"\\'ve\",\"\\'re\", \"n't\", 'INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "                  'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ', 'infj', 'entp','intp','intj',\n",
    "                  'entj','enfj','infp', 'enfp','isfp','istp','isfj','istj','estp','esfp','estj','esfj'}\n",
    "    stopwords_ = stopwords1.union(stopwords2)\n",
    "    \n",
    "    punctuation_ = set(string.punctuation)\n",
    "    return([w for w in sentence if not w in stopwords_ and not w in punctuation_])\n",
    "\n",
    "def remove_digits(sentences):\n",
    "    '''\n",
    "    remove the numbers in the text\n",
    "    '''\n",
    "    sentences_ = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r'[^a-zA-Z]+', ' ', sentence)\n",
    "        sentences_.append(sentence)\n",
    "    return [sent for sent in sentences_ if not sent in {\"\", \"'\"}]\n",
    "\n",
    "def lemm_and_stem(sentences):\n",
    "    '''\n",
    "    lemmatizing and stemming the words\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    output = []\n",
    "    for word_list in sentences:\n",
    "        lemmatized_words = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "        stemmed_words = ' '.join([stemmer.stem(w) for w in lemmatized_words])\n",
    "        output.append(stemmed_words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_str):\n",
    "    accent_removed=remove_accents(input_str) #remove the accent\n",
    "    sentences = accent_removed.split('|||') #split the string and get sentences\n",
    "    removed_http = remove_http(sentences)\n",
    "    removed_digits = remove_digits(removed_http)\n",
    "    tokens = [sent for sent in map(word_tokenize, removed_digits)]\n",
    "    tokens_lower = [[word.lower() for word in sentence]\n",
    "                 for sentence in tokens]\n",
    "    \n",
    "    tokens_filtered = list(map(filter_tokens, tokens_lower))\n",
    "    tokens_lemms = lemm_and_stem(tokens_filtered)\n",
    "\n",
    "    return tokens_lemms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_no_na[\"essay\"]=  users_no_na[\"essay\"].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_no_na[\"essay\"] = users_no_na[\"essay\"].apply(lambda x: x[0] if x !=[] else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_no_na = pd.DataFrame(users_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_no_na.to_csv(\"users_essay\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sorted(tfidf.vocabulary_)))\n",
    "# print(document_tfidf_matrix.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_lst = [\"aa\", \"bb\", \"cc\" , \"dd\" , \"ee\", \"ff\" , \"gg\", \"hh\", \"ii\" , \"jj\" , \"kk\", \"ll\", \"mm\", \"nn\", \"oo\", \"pp\", \"qq\", \"rr\", \"ss\", \"tt\", \"uu\", \"vv\", \"ww\",\"xx\", \"yy\", \"zz\"]\n",
    "data_dtm_cols = data_dtm.columns\n",
    "for col in cols_lst:\n",
    "    for column in data_dtm_cols:\n",
    "        if column.startswith(col):\n",
    "            print(column)\n",
    "            data_dtm.drop(column, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "rf_data = df[[\"age\", \"drinks\", \"education\", \"ethnicity\", \"orientation\", \"sex\", \"smokes\", \"religion\", \"drugs\", \"essay\"]]\n",
    "# rf_data = users_no_na[[\"drinks\",\"smokes\", \"drugs\"]]\n",
    "\n",
    "rf_data[\"drugs\"] = rf_data[\"drugs\"].replace([\"never\",\"sometimes\", \"often\"], [0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# putting different ethnicities in wider buckets\n",
    "\n",
    "rf_data[\"ethnicity\"] = rf_data[\"ethnicity\"].fillna(\"other\")\n",
    "rf_data.loc[rf_data['ethnicity'].str.contains('asian',), 'ethnicity'] = 'asian'\n",
    "rf_data.loc[rf_data['ethnicity'].str.contains('indian'), 'ethnicity'] = 'asian'\n",
    "rf_data.loc[rf_data[\"ethnicity\"].str.contains('hispanic / latin'), 'ethnicity'] = 'hispanic/latin'\n",
    "rf_data.loc[rf_data[\"ethnicity\"].str.contains('american'), 'ethnicity'] = 'american'\n",
    "rf_data.loc[rf_data[\"ethnicity\"].str.contains('native american'), 'ethnicity'] = 'american'\n",
    "rf_data.loc[rf_data['ethnicity'].str.contains('black'), 'ethnicity'] = 'american'\n",
    "rf_data.loc[rf_data[\"ethnicity\"].str.contains('pacific islander'), 'ethnicity'] = 'pacific islander'\n",
    "rf_data.loc[rf_data[\"ethnicity\"].str.contains('white'), 'ethnicity'] = 'american'\n",
    "rf_data.loc[rf_data[\"ethnicity\"].str.contains('middle eastern,'), 'ethnicity'] = 'middle eastern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# putting different religions in wider buckets\n",
    "\n",
    "rf_data[\"religion\"] = rf_data[\"religion\"].fillna(\"other\")\n",
    "rf_data.loc[rf_data['religion'].str.contains('agnosticism'), 'religion'] = 'agnosticism'\n",
    "rf_data.loc[rf_data['religion'].str.contains('atheism'), 'religion'] = 'atheism'\n",
    "rf_data.loc[rf_data['religion'].str.contains('catholicism'), 'religion'] = 'catholicism'\n",
    "rf_data.loc[rf_data['religion'].str.contains('buddhism'), 'religion'] = 'buddhism'\n",
    "rf_data.loc[rf_data['religion'].str.contains('other'), 'religion'] = 'other'\n",
    "rf_data.loc[rf_data['religion'].str.contains('hinduism'), 'religion'] = 'hinduism'\n",
    "rf_data.loc[rf_data['religion'].str.contains('islam'), 'religion'] = 'islam'\n",
    "rf_data.loc[rf_data['religion'].str.contains('judaism'), 'religion'] = 'judaism'\n",
    "rf_data.loc[rf_data['religion'].str.contains('christianity'), 'religion'] = 'christianity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting different age in wider buckets\n",
    "\n",
    "# rf_data[\"age\"] = [\"less than 30\" if val<=30 else \" above 30\" for val in rf_data[\"age\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# putting different education classes in wider buckets\n",
    "\n",
    "\n",
    "rf_data[\"education\"] = rf_data[\"education\"].fillna('in college/university')\n",
    "rf_data.loc[rf_data['education'].str.contains(\"graduated\"), 'education'] = 'graduated'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"working\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"two-year college\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"masters program\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"law school\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"space camp\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"ph.d program\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"med school\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"college/university\"), 'education'] = 'in college/university'\n",
    "rf_data.loc[rf_data['education'].str.contains(\"dropped out\"), 'education'] = 'dropped out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/shwetasharma1/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#treating nans\n",
    "rf_data[\"drinks\"] = rf_data[\"drinks\"].fillna(\"not at all\")\n",
    "rf_data[\"smokes\"] = rf_data[\"smokes\"].fillna(\"no\")\n",
    "# rf_data[\"smokes\"] = rf_data[\"smokes\"].fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rf_data = rf_data[[\"age\", \"drinks\", \"education\", \"ethnicity\", \"orientation\", \"sex\", \"smokes\", \"religion\", \"drugs\"]]\n",
    "cleaned_rf_data_dummified = pd.get_dummies(cleaned_rf_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rf_data_dummified = pd.concat([cleaned_rf_data_dummified, rf_data[\"essay\"]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45866, 38)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_rf_data_dummified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf = cleaned_rf_data_dummified.loc[:,cleaned_rf_data_dummified.columns != \"drugs\"]\n",
    "\n",
    "\n",
    "y_rf = cleaned_rf_data_dummified.loc[:,cleaned_rf_data_dummified.columns == \"drugs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf_train, X_rf_test, y_rf_train, y_rf_test = train_test_split(X_rf, y_rf, test_size = .10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = cleaned_rf_data_dummified['drugs'].value_counts()\n",
    "\n",
    "rf_class_0 = cleaned_rf_data_dummified[cleaned_rf_data_dummified['drugs'] == 0]\n",
    "rf_class_1 = cleaned_rf_data_dummified[cleaned_rf_data_dummified['drugs'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    37724\n",
      "0    37724\n",
      "Name: drugs, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_class_1_over = rf_class_1.sample(count_class_0, replace=True)\n",
    "rf_test_over = pd.concat([rf_class_0, rf_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(rf_test_over[\"drugs\"].value_counts())\n",
    "\n",
    "rf_test_over[\"drugs\"].value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf_over = rf_test_over.loc[:, rf_test_over.columns != \"drugs\"]\n",
    "y_rf_over = rf_test_over.loc[:, rf_test_over.columns==\"drugs\"]\n",
    "X_rf_over_train, X_rf_over_test, y_rf_over_train, y_rf_over_test = train_test_split(X_rf_over, y_rf_over[\"drugs\"], test_size=0.10, random_state=1)\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X_rf_over_train, y_rf_over_train)\n",
    "# y_rf_over_preds = model.predict(X_rf_over_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_matrix = tfidf.fit_transform(X_rf_over_train.essay.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_matrix_test =  tfidf.transform(X_rf_over_test.essay.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.06639615, ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tfidf_matrix_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm = pd.DataFrame(document_tfidf_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_test = pd.DataFrame(document_tfidf_matrix_test.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7545, 500)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dtm_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7545, 37)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rf_over_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.reset_index(drop=True, inplace=True)\n",
    "X_rf_over_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_nlp = pd.concat([data_dtm, X_rf_over_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_test.reset_index(drop=True, inplace=True)\n",
    "X_rf_over_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_nlp_test = pd.concat([data_dtm_test, X_rf_over_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>activ</th>\n",
       "      <th>actual</th>\n",
       "      <th>admit</th>\n",
       "      <th>adventur</th>\n",
       "      <th>age</th>\n",
       "      <th>ago</th>\n",
       "      <th>air</th>\n",
       "      <th>almost</th>\n",
       "      <th>along</th>\n",
       "      <th>...</th>\n",
       "      <th>smokes_yes</th>\n",
       "      <th>religion_agnosticism</th>\n",
       "      <th>religion_atheism</th>\n",
       "      <th>religion_buddhism</th>\n",
       "      <th>religion_catholicism</th>\n",
       "      <th>religion_christianity</th>\n",
       "      <th>religion_hinduism</th>\n",
       "      <th>religion_islam</th>\n",
       "      <th>religion_judaism</th>\n",
       "      <th>religion_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072748</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67898</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67899</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078568</td>\n",
       "      <td>0.201587</td>\n",
       "      <td>0.069099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67900</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67901</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67902</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67903 rows Ã— 536 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            abl  activ    actual     admit  adventur  age       ago       air  \\\n",
       "0      0.028982    0.0  0.023114  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "1      0.000000    0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.164461   \n",
       "2      0.000000    0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "3      0.000000    0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "4      0.000000    0.0  0.000000  0.000000  0.127390  0.0  0.000000  0.000000   \n",
       "...         ...    ...       ...       ...       ...  ...       ...       ...   \n",
       "67898  0.000000    0.0  0.012806  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "67899  0.000000    0.0  0.078568  0.201587  0.069099  0.0  0.000000  0.000000   \n",
       "67900  0.000000    0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "67901  0.000000    0.0  0.000000  0.000000  0.099109  0.0  0.000000  0.000000   \n",
       "67902  0.000000    0.0  0.000000  0.000000  0.000000  0.0  0.100565  0.000000   \n",
       "\n",
       "         almost     along  ...  smokes_yes  religion_agnosticism  \\\n",
       "0      0.000000  0.000000  ...           0                     1   \n",
       "1      0.000000  0.072748  ...           0                     0   \n",
       "2      0.000000  0.000000  ...           0                     1   \n",
       "3      0.105375  0.000000  ...           0                     0   \n",
       "4      0.000000  0.000000  ...           0                     0   \n",
       "...         ...       ...  ...         ...                   ...   \n",
       "67898  0.000000  0.014414  ...           0                     0   \n",
       "67899  0.000000  0.000000  ...           0                     0   \n",
       "67900  0.000000  0.000000  ...           0                     0   \n",
       "67901  0.000000  0.000000  ...           0                     0   \n",
       "67902  0.000000  0.000000  ...           1                     0   \n",
       "\n",
       "       religion_atheism  religion_buddhism  religion_catholicism  \\\n",
       "0                     0                  0                     0   \n",
       "1                     0                  0                     0   \n",
       "2                     0                  0                     0   \n",
       "3                     0                  0                     0   \n",
       "4                     0                  0                     0   \n",
       "...                 ...                ...                   ...   \n",
       "67898                 1                  0                     0   \n",
       "67899                 0                  0                     0   \n",
       "67900                 0                  0                     0   \n",
       "67901                 0                  0                     0   \n",
       "67902                 0                  0                     0   \n",
       "\n",
       "       religion_christianity  religion_hinduism  religion_islam  \\\n",
       "0                          0                  0               0   \n",
       "1                          0                  0               0   \n",
       "2                          0                  0               0   \n",
       "3                          0                  0               0   \n",
       "4                          0                  0               0   \n",
       "...                      ...                ...             ...   \n",
       "67898                      0                  0               0   \n",
       "67899                      0                  0               0   \n",
       "67900                      0                  0               0   \n",
       "67901                      0                  0               0   \n",
       "67902                      0                  0               0   \n",
       "\n",
       "       religion_judaism  religion_other  \n",
       "0                     0               0  \n",
       "1                     0               1  \n",
       "2                     0               0  \n",
       "3                     0               1  \n",
       "4                     0               1  \n",
       "...                 ...             ...  \n",
       "67898                 0               0  \n",
       "67899                 0               1  \n",
       "67900                 0               1  \n",
       "67901                 0               1  \n",
       "67902                 0               1  \n",
       "\n",
       "[67903 rows x 536 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nlp_test.drop(\"essay\", axis=1)\n",
    "df_nlp.drop(\"essay\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nlp.drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Passing list-likes to .loc or [] with any missing labels is no longer supported, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-8eef4f60ceec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rf_over_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9876e8e06f34>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(estimator, X, y)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"integer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1951\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1595\u001b[0m             return self.obj._reindex_with_indexers(\n\u001b[1;32m   1596\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         )\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 1654\u001b[0;31m                     \u001b[0;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m                     \u001b[0;34m\"is no longer supported, see \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                     \u001b[0;34m\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"\u001b[0m  \u001b[0;31m# noqa:E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Passing list-likes to .loc or [] with any missing labels is no longer supported, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike'"
     ]
    }
   ],
   "source": [
    "estimators = [LogisticRegression(max_iter=1000), RandomForestClassifier(), GradientBoostingClassifier()]\n",
    "for estimator in estimators:\n",
    "    run_model(estimator, df_nlp.values, y_rf_over_train)\n",
    "    \n",
    "plt.ylabel('Accuracy', fontsize=20)\n",
    "plt.xlabel('# of Folds', fontsize=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.824\n",
      "Precision: 0.813\n",
      "Recall: 0.842\n",
      "F1 score: 0.827\n"
     ]
    }
   ],
   "source": [
    "def generate_model_report(y_actual, y_predicted):\n",
    "    print('Accuracy: %.3f' % accuracy_score(y_actual, y_predicted))\n",
    "    print('Precision: %.3f' % precision_score(y_actual, y_predicted))\n",
    "    print( 'Recall: %.3f' % recall_score(y_actual, y_predicted))\n",
    "    print('F1 score: %.3f' % f1_score(y_actual, y_predicted))\n",
    "\n",
    "generate_model_report(y_rf_over_test, y_rf_over_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865\n",
      "Precision: 0.573\n",
      "Recall: 0.823\n",
      "F1 score: 0.675\n"
     ]
    }
   ],
   "source": [
    "y_final_preds = model.predict(X_rf_test)\n",
    "generate_model_report(y_rf_test, y_final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(estimator, X, y):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    acc = []\n",
    "    f1 = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train = X[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "         \n",
    "        \n",
    "        X_test = X[test_idx]\n",
    "        y_test = y[test_idx]\n",
    "\n",
    "        # data prep\n",
    "        \n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_preds = estimator.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test, y_preds))\n",
    "        f1.append(f1_score(y_test, y_preds))\n",
    "    \n",
    "    generate_model_report(y_test, y_preds)\n",
    "        \n",
    "    plt.plot(range(0, 10), f1, label=estimator.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators = [RandomForestClassifier()]\n",
    "estimators = [LogisticRegression(max_iter=1000), RandomForestClassifier(), GradientBoostingClassifier()]\n",
    "for estimator in estimators:\n",
    "    run_model(estimator, X_rf_over.values, Y_rf_over[\"drugs\"].values)\n",
    "    \n",
    "plt.ylabel('Accuracy', fontsize=20)\n",
    "plt.xlabel('# of Folds', fontsize=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_rf_over_train, y_rf_over_train)\n",
    "y_final_preds = model.predict(X_rf_test)\n",
    "generate_model_report(y_rf_test, y_final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "feat_scores = pd.DataFrame({'Fraction of Samples Affected' : model.feature_importances_},\n",
    "                           index=X_rf.columns)\n",
    "feat_scores = feat_scores.sort_values(by='Fraction of Samples Affected')\n",
    "feat_scores.plot(kind='barh')\n",
    "plt.title('RF Feature Importances - MDI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_report(y_actual, y_predicted):\n",
    "    print('Accuracy: %.3f' % accuracy_score(y_actual, y_predicted))\n",
    "    print('Precision: %.3f' % precision_score(y_actual, y_predicted))\n",
    "    print( 'Recall: %.3f' % recall_score(y_actual, y_predicted))\n",
    "    print('F1 score: %.3f' % f1_score(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def display_default_and_gsearch_model_results(model_default, model_gridsearch, \n",
    "                                              X_test, y_test):\n",
    "    '''\n",
    "        Parameters: model_default: fit model using initial parameters\n",
    "                    model_gridsearch: fit model using parameters from gridsearch\n",
    "                    X_test: 2d numpy array\n",
    "                    y_test: 1d numpy array\n",
    "        Return: None, but prints out mse and r2 for the default and model with\n",
    "                gridsearched parameters\n",
    "    '''\n",
    "    name = model_default.__class__.__name__.replace('Classifier', '') # for printing\n",
    "    y_test_pred = model_gridsearch.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Results for {0}\".format(name))\n",
    "    print(\"Gridsearched model acc: {:0.3f}\".format(acc))\n",
    "    y_test_pred = model_default.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"     Default model acc: {:0.3f}\".format(acc))\n",
    "    \n",
    "    \n",
    "    \n",
    "def gridsearch_with_output(estimator, parameter_grid, X_train, y_train):\n",
    "    '''\n",
    "        Parameters: estimator: the type of model (e.g. RandomForestRegressor())\n",
    "                    paramter_grid: dictionary defining the gridsearch parameters\n",
    "                    X_train: 2d numpy array\n",
    "                    y_train: 1d numpy array\n",
    "        Returns:  best parameters and model fit with those parameters\n",
    "    '''\n",
    "    model_gridsearch = GridSearchCV(estimator,\n",
    "                                    parameter_grid,\n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=True,\n",
    "                                    scoring='accuracy')\n",
    "    model_gridsearch.fit(X_train, y_train)\n",
    "    best_params = model_gridsearch.best_params_ \n",
    "    model_best = model_gridsearch.best_estimator_\n",
    "    print(\"\\nResult of gridsearch:\")\n",
    "    print(\"{0:<20s} | {1:<8s} | {2}\".format(\"Parameter\", \"Optimal\", \"Gridsearch values\"))\n",
    "    print(\"-\" * 55)\n",
    "    for param, vals in parameter_grid.items():\n",
    "        print(\"{0:<20s} | {1:<8s} | {2}\".format(str(param), \n",
    "                                                str(best_params[param]),\n",
    "                                                str(vals)))\n",
    "    return best_params, model_best\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=500, max_depth=2, random_state=1, max_features=1, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gb.fit(X_rf_over_train, y_rf_over_train)\n",
    "y_rf_over_preds = model.predict(X_rf_over_test)\n",
    "generate_model_report(y_rf_over_test, y_rf_over_preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final_preds = gb.predict(X_rf_test)\n",
    "generate_model_report(y_rf_test, y_final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_grid = {'max_depth': [3, None],\n",
    "                      'max_features': ['sqrt', 'log2', None],\n",
    "                      'min_samples_split': [2, 4],\n",
    "                      'min_samples_leaf': [1, 2, 4],\n",
    "                      'bootstrap': [True, False],\n",
    "                      'n_estimators': [10, 20, 40, 80],\n",
    "                      'random_state': [1]}\n",
    "rf_best_params, rf_best_model = gridsearch_with_output(RandomForestClassifier(), \n",
    "                                                       random_forest_grid, \n",
    "                                                       X_rf_over_train, y_rf_over_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_features=\"sqrt\", min_samples_split=2, min_samples_leaf =1, bootstrap = True, n_estimators=10, random_state=1)\n",
    "\n",
    "rf.fit(X_rf_over_train, y_rf_over_train)\n",
    "y_rf_over_preds = model.predict(X_rf_over_test)\n",
    "generate_model_report(y_rf_over_test, y_rf_over_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final_preds = rf.predict(X_rf_test)\n",
    "generate_model_report(y_rf_test, y_final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
